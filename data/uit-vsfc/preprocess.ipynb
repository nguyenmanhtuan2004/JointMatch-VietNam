{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>synonym_aug</th>\n",
       "      <th>back_translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>lớp này không phải wzjwz124 hướng dẫn thực hành .</td>\n",
       "      <td>vỉa này không phải wzjwz124 hướng_dẫn thực_hành .</td>\n",
       "      <td>Lớp học này không phải là ezzzz được thực tế h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>cần nhiều hoạt động ngoại khóa .</td>\n",
       "      <td>cần nhiều hoạt_động ngoại_khóa .</td>\n",
       "      <td>Rất nhiều hoạt động ngoại khóa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>kiến thức còn kỳ kỳ .</td>\n",
       "      <td>kiến_thức còn kỳ_kỳ .</td>\n",
       "      <td>Có thời gian cho tri thức.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>không có gì để phàn nàn .</td>\n",
       "      <td>không có gì để phàn_nàn .</td>\n",
       "      <td>Không có gì phải phàn nàn cả.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>cần xem xét lại khi đặt ra những yêu cầu về ch...</td>\n",
       "      <td>cần xem_xét lại khi đặt ra những yêu_cầu về tạ...</td>\n",
       "      <td>để xem xét các quy định cho các nhóm đối xứng,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content  \\\n",
       "0      3  lớp này không phải wzjwz124 hướng dẫn thực hành .   \n",
       "1      3                   cần nhiều hoạt động ngoại khóa .   \n",
       "2      1                              kiến thức còn kỳ kỳ .   \n",
       "3      3                          không có gì để phàn nàn .   \n",
       "4      1  cần xem xét lại khi đặt ra những yêu cầu về ch...   \n",
       "\n",
       "                                         synonym_aug  \\\n",
       "0  vỉa này không phải wzjwz124 hướng_dẫn thực_hành .   \n",
       "1                   cần nhiều hoạt_động ngoại_khóa .   \n",
       "2                              kiến_thức còn kỳ_kỳ .   \n",
       "3                          không có gì để phàn_nàn .   \n",
       "4  cần xem_xét lại khi đặt ra những yêu_cầu về tạ...   \n",
       "\n",
       "                                    back_translation  \n",
       "0  Lớp học này không phải là ezzzz được thực tế h...  \n",
       "1                    Rất nhiều hoạt động ngoại khóa.  \n",
       "2                         Có thời gian cho tri thức.  \n",
       "3                      Không có gì phải phàn nàn cả.  \n",
       "4  để xem xét các quy định cho các nhóm đối xứng,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Load stop words\n",
    "stop_words = []\n",
    "with open(\"../../text_augmentation_vietnamese/vietnamese-stopwords.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.append(line.strip('\\n'))\n",
    "\n",
    "def get_only_chars(line):\n",
    "    return line\n",
    "\n",
    "def normalize(text):\n",
    "    return text.strip().replace(\"_\", \" \").lower()\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    with open(\"../../text_augmentation_vietnamese/word_net_vi.json\", \"r\") as f:\n",
    "        wordnet = json.load(f)\n",
    "    for key, value in wordnet.items():\n",
    "        if normalize(key) == normalize(word):\n",
    "            for v in value:\n",
    "                synonyms.add(normalize(v))\n",
    "    if normalize(word) in synonyms:\n",
    "        synonyms.remove(normalize(word))\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "    return new_words\n",
    "\n",
    "def random_deletion(words, p):\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if random.uniform(0, 1) > p:\n",
    "            new_words.append(word)\n",
    "    if len(new_words) == 0:\n",
    "        return [words[random.randint(0, len(words) - 1)]]\n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        if len(new_words) > 0:\n",
    "            new_words = swap_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words) - 1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words) - 1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "    return new_words\n",
    "\n",
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1 and len(new_words) > 0:\n",
    "        random_word = new_words[random.randint(0, len(new_words) - 1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    if len(new_words) > 0 and len(synonyms) > 0:\n",
    "        random_synonym = synonyms[0]\n",
    "        random_idx = random.randint(0, len(new_words) - 1)\n",
    "        new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    sentence = get_only_chars(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word != '']\n",
    "    num_words = len(words)\n",
    "    augmented_sentences = []\n",
    "    if len(words) <= 0:\n",
    "        return augmented_sentences\n",
    "    num_new_per_technique = int(num_aug / 4) + 1\n",
    "    n_sr = max(1, int(alpha_sr * num_words))\n",
    "    n_ri = max(1, int(alpha_ri * num_words))\n",
    "    n_rs = max(1, int(alpha_rs * num_words))\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insertion(words, n_ri)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "    augmented_sentences = list(set(augmented_sentences))\n",
    "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "    random.shuffle(augmented_sentences)\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "    augmented_sentences.append(sentence)\n",
    "    return augmented_sentences\n",
    "\n",
    "def gen_eda(train_orig, output_file, alpha, num_aug=9):\n",
    "    writer = open(output_file, 'w')\n",
    "    lines = open(train_orig, 'r').readlines()\n",
    "    writer.write(\"free_text\" + \",\" + \"label_id\" + '\\n')\n",
    "    augm = \"\"\n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            parts = line[:-1].split('|')\n",
    "            label = parts[1]\n",
    "            sentence = parts[0]\n",
    "            aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
    "            for aug_sentence in aug_sentences:\n",
    "                augm = augm + aug_sentence + \",\" + label + '\\n'\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(parts)\n",
    "            pass\n",
    "    writer.write(augm)\n",
    "    writer.close()\n",
    "    print(\n",
    "        \"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(\n",
    "            num_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Weak Augmentation: Synonym Replacement\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "def synonym_aug_sentence(sentence):\n",
    "    # Tách từ tiếng Việt\n",
    "    words = word_tokenize(sentence, format=\"text\").split()\n",
    "    n_sr = max(1, int(0.1 * len(words)))\n",
    "    aug_words = synonym_replacement(words, n_sr)\n",
    "    return ' '.join(aug_words)\n",
    "\n",
    "df['synonym_aug'] = df['content'].apply(synonym_aug_sentence)\n",
    "df.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Anaconda3\\envs\\jointmatch\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/400 [00:00<?, ?it/s]c:\\Users\\HP\\Anaconda3\\envs\\jointmatch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3668: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n",
      "\n",
      "If you either need to use different keyword arguments for the source and target texts, you should do two calls like\n",
      "this:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "labels = tokenizer(text_target=tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n",
      "c:\\Users\\HP\\Anaconda3\\envs\\jointmatch\\lib\\site-packages\\transformers\\generation_utils.py:1227: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 400/400 [11:33<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu file train.csv với cột back_translation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Chuẩn bị mô hình dịch Việt-Anh và Anh-Việt\n",
    "vi_en_model_name = 'Helsinki-NLP/opus-mt-vi-en'\n",
    "en_vi_model_name = 'Helsinki-NLP/opus-mt-en-vi'\n",
    "\n",
    "vi_en_tokenizer = MarianTokenizer.from_pretrained(vi_en_model_name)\n",
    "vi_en_model = MarianMTModel.from_pretrained(vi_en_model_name)\n",
    "\n",
    "en_vi_tokenizer = MarianTokenizer.from_pretrained(en_vi_model_name)\n",
    "en_vi_model = MarianMTModel.from_pretrained(en_vi_model_name)\n",
    "\n",
    "def translate(text, tokenizer, model):\n",
    "    batch = tokenizer.prepare_seq2seq_batch([text], return_tensors=\"pt\")\n",
    "    gen = model.generate(**batch)\n",
    "    return tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
    "\n",
    "def back_translate_vi(text):\n",
    "    en = translate(text, vi_en_tokenizer, vi_en_model)\n",
    "    vi_bt = translate(en, en_vi_tokenizer, en_vi_model)\n",
    "    return vi_bt\n",
    "\n",
    "# Ví dụ áp dụng với pandas DataFrame\n",
    "tqdm.pandas()\n",
    "df['back_translation'] = df['content'].progress_apply(back_translate_vi)\n",
    "df.to_csv('train.csv', index=False)\n",
    "print(\"Đã lưu file train.csv với cột back_translation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>synonym_aug</th>\n",
       "      <th>back_translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>lớp này không phải wzjwz124 hướng dẫn thực hành .</td>\n",
       "      <td>vỉa này không phải wzjwz124 hướng_dẫn thực_hành .</td>\n",
       "      <td>Lớp học này không phải là ezzzz được thực tế h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>cần nhiều hoạt động ngoại khóa .</td>\n",
       "      <td>cần nhiều vui chơi ngoại_khóa .</td>\n",
       "      <td>Rất nhiều hoạt động ngoại khóa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>kiến thức còn kỳ kỳ .</td>\n",
       "      <td>học thức còn kỳ_kỳ .</td>\n",
       "      <td>Có thời gian cho tri thức.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>không có gì để phàn nàn .</td>\n",
       "      <td>không có gì để phàn_nàn .</td>\n",
       "      <td>Không có gì phải phàn nàn cả.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>cần xem xét lại khi đặt ra những yêu cầu về ch...</td>\n",
       "      <td>cần xét đến lại khi đặt ra những yêu_cầu về tạ...</td>\n",
       "      <td>để xem xét các quy định cho các nhóm đối xứng,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content  \\\n",
       "0      3  lớp này không phải wzjwz124 hướng dẫn thực hành .   \n",
       "1      3                   cần nhiều hoạt động ngoại khóa .   \n",
       "2      1                              kiến thức còn kỳ kỳ .   \n",
       "3      3                          không có gì để phàn nàn .   \n",
       "4      1  cần xem xét lại khi đặt ra những yêu cầu về ch...   \n",
       "\n",
       "                                         synonym_aug  \\\n",
       "0  vỉa này không phải wzjwz124 hướng_dẫn thực_hành .   \n",
       "1                    cần nhiều vui chơi ngoại_khóa .   \n",
       "2                               học thức còn kỳ_kỳ .   \n",
       "3                          không có gì để phàn_nàn .   \n",
       "4  cần xét đến lại khi đặt ra những yêu_cầu về tạ...   \n",
       "\n",
       "                                    back_translation  \n",
       "0  Lớp học này không phải là ezzzz được thực tế h...  \n",
       "1                    Rất nhiều hoạt động ngoại khóa.  \n",
       "2                         Có thời gian cho tri thức.  \n",
       "3                      Không có gì phải phàn nàn cả.  \n",
       "4  để xem xét các quy định cho các nhóm đối xứng,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    100\n",
       "1    100\n",
       "2    100\n",
       "0    100\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "\n",
    "df_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dev.csv')  # Thay bằng tên file thực tế của bạn\n",
    "mapping = {4: 3, 3: 2, 2: 1, 1: 0}\n",
    "df['label'] = df['label'].map(mapping).fillna(df['label']).astype(int)\n",
    "df.to_csv('dev.csv', index=False)  # Lưu thành file mới\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Đã lưu file dev.csv cho mỗi lớp!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dev.csv')\n",
    "target_per_class = 20\n",
    "\n",
    "sampled_df = pd.concat([\n",
    "    df[df['label'] == k].sample(n=target_per_class, random_state=42)\n",
    "    for k in df['label'].unique()\n",
    "])\n",
    "\n",
    "sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "sampled_df.to_csv('dev.csv', index=False, encoding='utf-8')\n",
    "print(\"✅ Đã lưu file dev.csv cho mỗi lớp!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointmatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
